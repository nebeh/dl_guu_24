{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подается синусоида произвольной длины, случайным значением амплитуды и частоты. Задача: по входной последовательности определить значения аплитуды и частоты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(batch = 4):\n",
    "    t = np.arange(0,torch.randint(3,15,(1,)),step=0.1)\n",
    "    X = torch.zeros(batch,t.size,1)\n",
    "    Y = torch.zeros(batch,2)\n",
    "    for i in range(batch):\n",
    "        a = torch.rand(1)*3 + 0.1\n",
    "        b = torch.rand(1)*4 + 0.2\n",
    "        X[i,:,0] =  a*np.sin(b*t)\n",
    "        Y[i,0],Y[i,1] = a,b\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "class TransformerRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, nhead, hidden_dim, num_layers):\n",
    "        super(TransformerRegression, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        encoder_layer = TransformerEncoderLayer(hidden_dim, nhead)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)  # Изменяем форму для передачи в трансформер\n",
    "        output = self.encoder(x)  # Применяем трансформер\n",
    "        output = output[0,:,:]\n",
    "        output = self.fc(output)  # Полносвязный слой для предсказания\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/strike/penv/deep/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dim = 1\n",
    "output_dim = 2\n",
    "nhead = 2#4\n",
    "hidden_dim = 32\n",
    "num_layers = 2#4\n",
    "\n",
    "model = TransformerRegression(input_dim, output_dim, nhead, hidden_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/strike/penv/deep/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/4000], Loss: 0.8408\n",
      "Epoch [200/4000], Loss: 0.6741\n",
      "Epoch [300/4000], Loss: 0.5608\n",
      "Epoch [400/4000], Loss: 0.5582\n",
      "Epoch [500/4000], Loss: 0.4994\n",
      "Epoch [600/4000], Loss: 0.5361\n",
      "Epoch [700/4000], Loss: 0.4854\n",
      "Epoch [800/4000], Loss: 0.4851\n",
      "Epoch [900/4000], Loss: 0.5104\n",
      "Epoch [1000/4000], Loss: 0.4741\n",
      "Epoch [1100/4000], Loss: 0.4830\n",
      "Epoch [1200/4000], Loss: 0.4789\n",
      "Epoch [1300/4000], Loss: 0.4796\n",
      "Epoch [1400/4000], Loss: 0.4936\n",
      "Epoch [1500/4000], Loss: 0.4540\n",
      "Epoch [1600/4000], Loss: 0.4794\n",
      "Epoch [1700/4000], Loss: 0.4864\n",
      "Epoch [1800/4000], Loss: 0.4782\n",
      "Epoch [1900/4000], Loss: 0.4733\n",
      "Epoch [2000/4000], Loss: 0.4563\n",
      "Epoch [2100/4000], Loss: 0.5110\n",
      "Epoch [2200/4000], Loss: 0.4845\n",
      "Epoch [2300/4000], Loss: 0.5186\n",
      "Epoch [2400/4000], Loss: 0.4835\n",
      "Epoch [2500/4000], Loss: 0.4910\n",
      "Epoch [2600/4000], Loss: 0.4674\n",
      "Epoch [2700/4000], Loss: 0.4591\n",
      "Epoch [2800/4000], Loss: 0.4596\n",
      "Epoch [2900/4000], Loss: 0.4789\n",
      "Epoch [3000/4000], Loss: 0.4577\n",
      "Epoch [3100/4000], Loss: 0.4657\n",
      "Epoch [3200/4000], Loss: 0.4619\n",
      "Epoch [3300/4000], Loss: 0.4900\n",
      "Epoch [3400/4000], Loss: 0.4607\n",
      "Epoch [3500/4000], Loss: 0.4756\n",
      "Epoch [3600/4000], Loss: 0.4720\n",
      "Epoch [3700/4000], Loss: 0.4711\n",
      "Epoch [3800/4000], Loss: 0.4527\n",
      "Epoch [3900/4000], Loss: 0.4676\n",
      "Epoch [4000/4000], Loss: 0.4860\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "s = []\n",
    "# Обучение модели\n",
    "num_epochs = 4000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x,y = gen_data(batch=16)\n",
    "    output = model(x.to(device))\n",
    "    \n",
    "    loss = criterion(output, y.to(device))  # Используем только первый измерение для задачи регрессии\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    l = loss.detach().to('cpu')\n",
    "    s.append(l)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        L = np.mean(s)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {L:.4f}')\n",
    "        s = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4883)\n"
     ]
    }
   ],
   "source": [
    "x,y = gen_data(batch=32)\n",
    "output = model(x)\n",
    "loss = criterion(output, y.to(device))\n",
    "print(loss.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим позиционное кодирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "class TransformerRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, nhead, hidden_dim, num_layers):\n",
    "        super(TransformerRegression, self).__init__()\n",
    "        self.pos_enc = PositionalEncoding(hidden_dim,1000)\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        encoder_layer = TransformerEncoderLayer(hidden_dim, nhead)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_enc(x)\n",
    "        x = x.permute(1, 0, 2)  # Изменяем форму для передачи в трансформер\n",
    "        output = self.encoder(x)  # Применяем трансформер\n",
    "        output = output[0,:,:]\n",
    "        output = self.fc(output)  # Полносвязный слой для предсказания\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/strike/penv/deep/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model_pos_cod = TransformerRegression(input_dim, output_dim, nhead, hidden_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/4000], Loss: 0.8334\n",
      "Epoch [200/4000], Loss: 0.3511\n",
      "Epoch [300/4000], Loss: 0.0973\n",
      "Epoch [400/4000], Loss: 0.0653\n",
      "Epoch [500/4000], Loss: 0.0615\n",
      "Epoch [600/4000], Loss: 0.0705\n",
      "Epoch [700/4000], Loss: 0.0463\n",
      "Epoch [800/4000], Loss: 0.0375\n",
      "Epoch [900/4000], Loss: 0.0335\n",
      "Epoch [1000/4000], Loss: 0.0496\n",
      "Epoch [1100/4000], Loss: 0.0299\n",
      "Epoch [1200/4000], Loss: 0.0251\n",
      "Epoch [1300/4000], Loss: 0.0251\n",
      "Epoch [1400/4000], Loss: 0.0249\n",
      "Epoch [1500/4000], Loss: 0.0392\n",
      "Epoch [1600/4000], Loss: 0.0203\n",
      "Epoch [1700/4000], Loss: 0.0460\n",
      "Epoch [1800/4000], Loss: 0.0268\n",
      "Epoch [1900/4000], Loss: 0.0194\n",
      "Epoch [2000/4000], Loss: 0.0210\n",
      "Epoch [2100/4000], Loss: 0.0220\n",
      "Epoch [2200/4000], Loss: 0.0180\n",
      "Epoch [2300/4000], Loss: 0.0159\n",
      "Epoch [2400/4000], Loss: 0.0147\n",
      "Epoch [2500/4000], Loss: 0.0160\n",
      "Epoch [2600/4000], Loss: 0.0229\n",
      "Epoch [2700/4000], Loss: 0.0156\n",
      "Epoch [2800/4000], Loss: 0.0135\n",
      "Epoch [2900/4000], Loss: 0.0147\n",
      "Epoch [3000/4000], Loss: 0.0121\n",
      "Epoch [3100/4000], Loss: 0.0106\n",
      "Epoch [3200/4000], Loss: 0.0130\n",
      "Epoch [3300/4000], Loss: 0.0110\n",
      "Epoch [3400/4000], Loss: 0.0122\n",
      "Epoch [3500/4000], Loss: 0.0130\n",
      "Epoch [3600/4000], Loss: 0.0137\n",
      "Epoch [3700/4000], Loss: 0.0143\n",
      "Epoch [3800/4000], Loss: 0.0223\n",
      "Epoch [3900/4000], Loss: 0.0130\n",
      "Epoch [4000/4000], Loss: 0.0242\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_pos_cod.parameters(), lr=0.001)\n",
    "s = []\n",
    "# Обучение модели\n",
    "num_epochs = 4000\n",
    "for epoch in range(num_epochs):\n",
    "    model_pos_cod.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x,y = gen_data(batch=16)\n",
    "    output = model_pos_cod(x.to(device))\n",
    "    \n",
    "    loss = criterion(output, y.to(device))  # Используем только первый измерение для задачи регрессии\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    l = loss.detach().to('cpu')\n",
    "    s.append(l)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        L = np.mean(s)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {L:.4f}')\n",
    "        s = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0135)\n"
     ]
    }
   ],
   "source": [
    "x,y = gen_data(batch=32)\n",
    "output = model_pos_cod(x)\n",
    "loss = criterion(output, y.to(device))\n",
    "print(loss.detach())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
