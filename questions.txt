1. 	Однослойные и многослойные полносвязные сети для задач классификации и регрессии. Функции активации. Прямое и обратное распространение ошибки
2.  	Loss функции в задачах глубокого обучения. Метод максимального правдоподобия. Связь метода максимального правдоподобия и нормы L2
3.	Слои сверточных нейронных сетей, функционал, назначение, принципы работы
4.	Архитектуры сверточных нейронных сетей. Передача обучения
5.	Оптимизация: стохастический градиент,  Adam, RmsProp. Автоматическое дифференцирование. Матричные производные. 
6.	Повышение качества обучения (минимизация loss функции): аугментация, выбор гиперпараметров, настройка гиперпарамтеров, Байесовская оптимизация. Плюсы и минусы каждого подхода
7.       Метрические методы обучения. Сиамские сети. Функции ошибок для метрических методов
8. (*)	Автоэнкодеры: VAE, CVAE 
9.(*)	Генеративные модели: GAN, нормализующие потоки
10. (*) 	Дифузионные модели
11.	Детекция объектов: RCNN Fast RCNN, Faster RCNN
12.	Детекция объектов: YOLOv1, v2, v3
13.	Сегментация объектов: SegNet, Unet. DeepLab 

14.	Рекуррентные сети. Регрессия, авторегрессия, seq2seq. Устройство LSTM, biLSTM нейрона
15. 	Рекуррентные сети и механизм внимания в  seq2seq последовательностях
16. 	LSTM vs Тансформер. Достоинства и недостатки каждой архитектуры. Основные различия. 
17.       Тексты. Принципы предобработки текстов. Схема обучения word2vec. Что такое токен. Модель WordPiece. 
18. 	Машинный перевод. Модель языка. Схема обучения в машинном переводе. Метрики качества на примере  BLEU
19.       Трансформеры. Архитектура, функционал, элементы. Принцип работы
20. 	Внимание и самовнимание в трансформерах. Query, Keys, Values, Mask Attantion
21. 	 Архитектуры Декодер, Энкодер — Декодер, Энкодер. Сходство и различия. Примеры применения.
